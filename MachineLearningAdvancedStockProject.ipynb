{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjUr81qT8SVf",
        "outputId": "68eb87e1-0321-42e8-e0a6-eeccf339365f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully fetched!\n",
            "                       Date       Open       High        Low      Close  \\\n",
            "0 2015-04-06 00:00:00-04:00  27.797603  28.476520  27.766337  28.440786   \n",
            "1 2015-04-07 00:00:00-04:00  28.505539  28.612735  28.134816  28.141516   \n",
            "2 2015-04-08 00:00:00-04:00  28.105785  28.228616  27.909258  28.049953   \n",
            "3 2015-04-09 00:00:00-04:00  28.105790  28.268820  27.840032  28.264353   \n",
            "4 2015-04-10 00:00:00-04:00  28.128122  28.409515  27.974027  28.384949   \n",
            "\n",
            "      Volume Ticker  MA7  MA21  \n",
            "0  148776000   AAPL  NaN   NaN  \n",
            "1  140049200   AAPL  NaN   NaN  \n",
            "2  149316800   AAPL  NaN   NaN  \n",
            "3  129936000   AAPL  NaN   NaN  \n",
            "4  160752000   AAPL  NaN   NaN  \n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import concurrent.futures\n",
        "\n",
        "#10+ companies ke stock tickers (Modify as needed)\n",
        "tickers = [\"AAPL\", \"TSLA\", \"GOOGL\", \"MSFT\", \"AMZN\", \"NVDA\", \"META\", \"JPM\", \"NFLX\", \"AMD\", \"IBM\"]\n",
        "\n",
        "#Function to fetch stock data\n",
        "def fetch_stock_data(ticker):\n",
        "    \"\"\"\n",
        "    Fetches 10 years of daily historical stock data from Yahoo Finance.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        stock = yf.Ticker(ticker)\n",
        "        data = stock.history(period=\"10y\", interval=\"1d\")\n",
        "\n",
        "        if data.empty:\n",
        "            print(f\" No data found for {ticker}. Skipping...\")\n",
        "            return None\n",
        "\n",
        "        #Selecting important features\n",
        "        data = data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "        #Reset index to make 'Date' a column\n",
        "        data.reset_index(inplace=True)\n",
        "\n",
        "        #Add Ticker column to differentiate stocks\n",
        "        data[\"Ticker\"] = ticker\n",
        "\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {ticker}: {e}\")\n",
        "        return None\n",
        "\n",
        "#Fetch multiple stock data in parallel (10x Faster!)\n",
        "def fetch_multiple_stocks(tickers):\n",
        "    \"\"\"\n",
        "    Fetches historical data for multiple stocks efficiently using multi-threading.\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        results = executor.map(fetch_stock_data, tickers)\n",
        "\n",
        "    for result in results:\n",
        "        if result is not None:\n",
        "            all_data.append(result)\n",
        "\n",
        "    if all_data:\n",
        "        df = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "        #Data Cleaning (Remove Missing Values)\n",
        "        df.dropna(inplace=True)\n",
        "\n",
        "        #Feature Engineering: Adding Moving Averages (for better predictions)\n",
        "        df['MA7'] = df.groupby('Ticker')['Close'].rolling(window=7).mean().reset_index(0, drop=True)\n",
        "        df['MA21'] = df.groupby('Ticker')['Close'].rolling(window=21).mean().reset_index(0, drop=True)\n",
        "\n",
        "        return df\n",
        "    else:\n",
        "        print(\"No data retrieved for any ticker.\")\n",
        "        return None\n",
        "\n",
        "#Fetch and save data for multiple stocks\n",
        "stock_data = fetch_multiple_stocks(tickers)\n",
        "\n",
        "if stock_data is not None:\n",
        "    print(\"Data successfully fetched!\")\n",
        "    print(stock_data.head())  # Display first few rows\n",
        "    stock_data.to_csv(\"multi_stock_data_advanced.csv\", index=False)  # Save optimized dataset\n",
        "else:\n",
        "    print(\"Data fetching failed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4qz1FS-8qaD",
        "outputId": "06b2e047-ae34-4769-91bf-2a966e36ac15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-8b6abfc5097b>:10: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
            "  df['Date'] = pd.to_datetime(df['Date'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2 Completed: Feature Engineering & Data Preprocessing Done!\n",
            "                        Date      Open      High       Low     Close  \\\n",
            "0  2015-04-06 00:00:00-04:00  0.025796  0.026319  0.026223  0.026437   \n",
            "1  2015-04-07 00:00:00-04:00  0.026464  0.026447  0.026577  0.026154   \n",
            "2  2015-04-08 00:00:00-04:00  0.026087  0.026086  0.026360  0.026067   \n",
            "3  2015-04-09 00:00:00-04:00  0.026087  0.026124  0.026294  0.026270   \n",
            "4  2015-04-10 00:00:00-04:00  0.026108  0.026256  0.026423  0.026384   \n",
            "\n",
            "     Volume Ticker  MA7  MA21  Daily Return  Volatility  MA50  MA200  \n",
            "0  0.039989   AAPL  NaN   NaN           NaN         NaN   NaN    NaN  \n",
            "1  0.037625   AAPL  NaN   NaN      0.389722         NaN   NaN    NaN  \n",
            "2  0.040136   AAPL  NaN   NaN      0.398039         NaN   NaN    NaN  \n",
            "3  0.034886   AAPL  NaN   NaN      0.410506         NaN   NaN    NaN  \n",
            "4  0.043233   AAPL  NaN   NaN      0.406643         NaN   NaN    NaN  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"multi_stock_data_advanced.csv\"  # Step 1 ka saved dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert 'Date' column to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Sort values by Date (Important for time-series data)\n",
        "df = df.sort_values(by=['Ticker', 'Date'])\n",
        "\n",
        "# Feature Engineering: Creating New Indicators\n",
        "df['Daily Return'] = df.groupby('Ticker')['Close'].pct_change()  # Percentage Change\n",
        "df['Volatility'] = df.groupby('Ticker')['Daily Return'].rolling(window=7).std().reset_index(0, drop=True)  # 7-day rolling volatility\n",
        "\n",
        "#Moving Averages for Trend Detection\n",
        "df['MA50'] = df.groupby('Ticker')['Close'].rolling(window=50).mean().reset_index(0, drop=True)\n",
        "df['MA200'] = df.groupby('Ticker')['Close'].rolling(window=200).mean().reset_index(0, drop=True)\n",
        "\n",
        "#Normalization (Scaling for ML Models)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'MA7', 'MA21', 'MA50', 'MA200', 'Volatility', 'Daily Return']\n",
        "df[scaled_features] = scaler.fit_transform(df[scaled_features])\n",
        "\n",
        "#Save the preprocessed data\n",
        "df.to_csv(\"preprocessed_stock_data.csv\", index=False)\n",
        "\n",
        "print(\"Step 2 Completed: Feature Engineering & Data Preprocessing Done!\")\n",
        "print(df.head())  # Display first few rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_TUqwYi8qdb",
        "outputId": "5385f675-45bd-444f-9784-280516d527be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3 Completed: Train-Test Split Done!\n",
            "Training Data: (20398, 11), Testing Data: (5100, 11)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Load the preprocessed dataset\n",
        "file_path = \"preprocessed_stock_data.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "#Drop rows with NaN values (generated due to moving averages)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "#Define features (X) and target (y)\n",
        "features = ['Open', 'High', 'Low', 'Close', 'Volume', 'MA7', 'MA21', 'MA50', 'MA200', 'Volatility', 'Daily Return']\n",
        "target = 'Close'  # Predicting future Close price\n",
        "\n",
        "X = df[features].values\n",
        "y = df[target].values\n",
        "\n",
        "#Time-series based train-test split (80% Train, 20% Test)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(df) * split_ratio)\n",
        "\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "#Save the train-test data\n",
        "np.save(\"X_train.npy\", X_train)\n",
        "np.save(\"X_test.npy\", X_test)\n",
        "np.save(\"y_train.npy\", y_train)\n",
        "np.save(\"y_test.npy\", y_test)\n",
        "\n",
        "print(f\"Step 3 Completed: Train-Test Split Done!\")\n",
        "print(f\"Training Data: {X_train.shape}, Testing Data: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo0AWig48qh6",
        "outputId": "34b0a3f8-45c3-4919-8bab-4b10f44239d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - loss: 0.0025\n",
            "Epoch 2/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - loss: 3.1808e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 2.9374e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - loss: 2.6775e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 2.4998e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 2.2910e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - loss: 2.2072e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 2.3280e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 2.3351e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - loss: 2.2370e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - loss: 2.1190e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - loss: 2.0726e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - loss: 2.1301e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - loss: 2.0933e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.2881e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.1620e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 1.9294e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.9716e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - loss: 2.0824e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.1104e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.0268e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.0204e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - loss: 2.1407e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - loss: 1.8470e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.8794e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 11ms/step - loss: 1.8008e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.8434e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 12ms/step - loss: 1.8949e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - loss: 1.9548e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - loss: 1.8421e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 2.1094e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - loss: 1.9772e-04\n",
            "Epoch 33/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - loss: 1.8912e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - loss: 1.8829e-04\n",
            "Epoch 35/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - loss: 1.8127e-04\n",
            "Epoch 36/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - loss: 1.7260e-04\n",
            "Epoch 37/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.9240e-04\n",
            "Epoch 38/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.8294e-04\n",
            "Epoch 39/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - loss: 1.7564e-04\n",
            "Epoch 40/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.8254e-04\n",
            "Epoch 41/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.8591e-04\n",
            "Epoch 42/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.8226e-04\n",
            "Epoch 43/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 1.9352e-04\n",
            "Epoch 44/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.8076e-04\n",
            "Epoch 45/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - loss: 1.8453e-04\n",
            "Epoch 46/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.8758e-04\n",
            "Epoch 47/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - loss: 1.9531e-04\n",
            "Epoch 48/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - loss: 1.8484e-04\n",
            "Epoch 49/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 1.7363e-04\n",
            "Epoch 50/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - loss: 1.6261e-04\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 2.2376e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 4.2946e-06\n",
            "Epoch 3/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 4.3495e-06\n",
            "Epoch 4/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 3.6850e-06\n",
            "Epoch 5/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 3.2211e-06\n",
            "Epoch 6/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2.8992e-06\n",
            "Epoch 7/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.4216e-06\n",
            "Epoch 8/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.6679e-06\n",
            "Epoch 9/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.3666e-06\n",
            "Epoch 10/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.5324e-06\n",
            "Epoch 11/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 4.0719e-06\n",
            "Epoch 12/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.3142e-06\n",
            "Epoch 13/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.3554e-06\n",
            "Epoch 14/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.3692e-06\n",
            "Epoch 15/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.7982e-06\n",
            "Epoch 16/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1459e-06\n",
            "Epoch 17/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.7891e-06\n",
            "Epoch 18/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.4057e-06\n",
            "Epoch 19/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.7838e-06\n",
            "Epoch 20/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 7.8997e-07\n",
            "Epoch 21/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 2.5120e-06\n",
            "Epoch 22/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.3343e-06\n",
            "Epoch 23/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 8.8574e-07\n",
            "Epoch 24/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1053e-06\n",
            "Epoch 25/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 8.2311e-07\n",
            "Epoch 26/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.0649e-06\n",
            "Epoch 27/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 8.9470e-07\n",
            "Epoch 28/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.0490e-06\n",
            "Epoch 29/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 9.5312e-07\n",
            "Epoch 30/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.1263e-06\n",
            "Epoch 31/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 7.2624e-07\n",
            "Epoch 32/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 7.6435e-07\n",
            "Epoch 33/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 6.9652e-07\n",
            "Epoch 34/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 5.3812e-07\n",
            "Epoch 35/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 9.1424e-07\n",
            "Epoch 36/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 4.9268e-07\n",
            "Epoch 37/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 9.1560e-07\n",
            "Epoch 38/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 5.3483e-07\n",
            "Epoch 39/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 8.4621e-07\n",
            "Epoch 40/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 5.1833e-07\n",
            "Epoch 41/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 7.5493e-07\n",
            "Epoch 42/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 9.3519e-07\n",
            "Epoch 43/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 5.0495e-07\n",
            "Epoch 44/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 9.6571e-07\n",
            "Epoch 45/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 4.6943e-07\n",
            "Epoch 46/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 5.8303e-07\n",
            "Epoch 47/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 5.7869e-07\n",
            "Epoch 48/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 6.3531e-07\n",
            "Epoch 49/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 4.1592e-07\n",
            "Epoch 50/50\n",
            "\u001b[1m1275/1275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 3.5067e-07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004636 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2805\n",
            "[LightGBM] [Info] Number of data points in the train set: 20398, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 0.141026\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "LSTM - MAE: 0.0171, MSE: 0.0005\n",
            "CNN - MAE: 0.0036, MSE: 0.0000\n",
            "XGBoost - MAE: 0.0068, MSE: 0.0015\n",
            "Random Forest - MAE: 0.0040, MSE: 0.0008\n",
            "SVM - MAE: 0.0800, MSE: 0.0122\n",
            "LightGBM - MAE: 0.0066, MSE: 0.0014\n",
            "ARIMA - MAE: 0.2917, MSE: 0.0967\n",
            "Step 4 Completed: AI Models Training & Evaluation Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR\n",
        "import lightgbm as lgb\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Load train-test data\n",
        "X_train = np.load(\"X_train.npy\")\n",
        "X_test = np.load(\"X_test.npy\")\n",
        "y_train = np.load(\"y_train.npy\")\n",
        "y_test = np.load(\"y_test.npy\")\n",
        "\n",
        "#LSTM Model\n",
        "def build_lstm_model():\n",
        "    model = Sequential([\n",
        "        LSTM(50, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
        "        Dropout(0.2),\n",
        "        LSTM(50, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# CNN Model\n",
        "def build_cnn_model():\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Flatten(),\n",
        "        Dense(50, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Reshape for LSTM & CNN\n",
        "X_train_lstm = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test_lstm = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# Train LSTM Model\n",
        "lstm_model = build_lstm_model()\n",
        "lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=16, verbose=1)\n",
        "\n",
        "# Train CNN Model\n",
        "cnn_model = build_cnn_model()\n",
        "cnn_model.fit(X_train_lstm, y_train, epochs=50, batch_size=16, verbose=1)\n",
        "\n",
        "# Train XGBoost Model\n",
        "xgb = XGBRegressor(n_estimators=100, learning_rate=0.1)\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Train Random Forest Model\n",
        "rf = RandomForestRegressor(n_estimators=100)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Train SVM Model\n",
        "svm = SVR(kernel='rbf')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Train LightGBM Model\n",
        "lgb_model = lgb.LGBMRegressor(n_estimators=100)\n",
        "lgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Train ARIMA Model\n",
        "arima_order = (5,1,0)  # Change based on best parameters\n",
        "arima = ARIMA(y_train, order=arima_order)\n",
        "arima_fit = arima.fit()\n",
        "\n",
        "# Predictions\n",
        "y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
        "y_pred_cnn = cnn_model.predict(X_test_lstm)\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "y_pred_lgb = lgb_model.predict(X_test)\n",
        "y_pred_arima = arima_fit.forecast(steps=len(y_test))\n",
        "\n",
        "# Evaluate Models\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    print(f\"{model_name} - MAE: {mae:.4f}, MSE: {mse:.4f}\")\n",
        "\n",
        "evaluate_model(y_test, y_pred_lstm, \"LSTM\")\n",
        "evaluate_model(y_test, y_pred_cnn, \"CNN\")\n",
        "evaluate_model(y_test, y_pred_xgb, \"XGBoost\")\n",
        "evaluate_model(y_test, y_pred_rf, \"Random Forest\")\n",
        "evaluate_model(y_test, y_pred_svm, \"SVM\")\n",
        "evaluate_model(y_test, y_pred_lgb, \"LightGBM\")\n",
        "evaluate_model(y_test, y_pred_arima, \"ARIMA\")\n",
        "\n",
        "print(\"Step 4 Completed: AI Models Training & Evaluation Done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUutfmHg8q1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d47e99-a1e1-46d5-9fb3-380dcd2d2c37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Best Model Selected: CNN\n",
            "Step 5 Completed: Model Files Saved for Backend Deployment!\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import warnings\n",
        "\n",
        "# Suppress FutureWarnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Load previous results\n",
        "models = {\n",
        "    \"LSTM\": lstm_model,\n",
        "    \"CNN\": cnn_model,\n",
        "    \"XGBoost\": xgb,\n",
        "    \"Random Forest\": rf,\n",
        "    \"SVM\": svm,\n",
        "    \"LightGBM\": lgb_model,\n",
        "    \"ARIMA\": arima_fit\n",
        "}\n",
        "\n",
        "# Reshape X_test for LSTM & CNN models\n",
        "X_test_lstm = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # For LSTM\n",
        "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))   # For CNN\n",
        "\n",
        "# Get predictions for all models\n",
        "predictions = {\n",
        "    \"LSTM\": lstm_model.predict(X_test_lstm),\n",
        "    \"CNN\": cnn_model.predict(X_test_cnn),\n",
        "    \"XGBoost\": xgb.predict(X_test),\n",
        "    \"Random Forest\": rf.predict(X_test),\n",
        "    \"SVM\": svm.predict(X_test),\n",
        "    \"LightGBM\": lgb_model.predict(X_test),\n",
        "    \"ARIMA\": arima_fit.forecast(steps=len(X_test))  # ARIMA needs `.forecast()`\n",
        "}\n",
        "\n",
        "# Select Best Model (Based on lowest MAE)\n",
        "best_model_name = min(predictions.keys(), key=lambda x: mean_absolute_error(y_test, predictions[x]))\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "print(f\"Best Model Selected: {best_model_name}\")\n",
        "\n",
        "# Save models for backend deployment\n",
        "joblib.dump(xgb, \"xgb_model.pkl\")\n",
        "joblib.dump(rf, \"rf_model.pkl\")\n",
        "joblib.dump(svm, \"svm_model.pkl\")\n",
        "joblib.dump(lgb_model, \"lgb_model.pkl\")\n",
        "\n",
        "#Use `.keras` format instead of `.h5`\n",
        "lstm_model.save(\"lstm_model.keras\")\n",
        "cnn_model.save(\"cnn_model.keras\")\n",
        "\n",
        "# Save ARIMA model (different format)\n",
        "joblib.dump(arima_fit, \"arima_model.pkl\")\n",
        "\n",
        "print(\"Step 5 Completed: Model Files Saved for Backend Deployment!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import pickle\n",
        "import plotly.graph_objects as go\n",
        "from keras.models import load_model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#Models Load Karein\n",
        "MODEL_PATH = \"model/\"\n",
        "lstm_model = load_model(MODEL_PATH + \"lstm_model.h5\")\n",
        "xgb_model = pickle.load(open(MODEL_PATH + \"xgb_model.pkl\", \"rb\"))\n",
        "arima_model = pickle.load(open(MODEL_PATH + \"arima_model.pkl\", \"rb\"))\n",
        "\n",
        "#Function: Stock Data Fetch Karna\n",
        "def get_stock_data(company, period=\"5y\"):\n",
        "    data = yf.download(company, period=period)\n",
        "    if data.empty:\n",
        "        raise ValueError(\"No stock data found. Please check the ticker symbol.\")\n",
        "    return data\n",
        "\n",
        "#Function: Data Preprocessing\n",
        "def preprocess_data(data):\n",
        "    if data.empty:\n",
        "        raise ValueError(\"No data available for processing.\")\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "    data_scaled = scaler.fit_transform(data['Close'].values.reshape(-1,1))\n",
        "    return data_scaled, scaler\n",
        "\n",
        "#Function: Predict Stock Price\n",
        "def predict_stock(company):\n",
        "    data = get_stock_data(company)\n",
        "    data_scaled, scaler = preprocess_data(data)\n",
        "\n",
        "    #LSTM Prediction\n",
        "    last_100 = data_scaled[-100:].reshape(1, -1, 1)\n",
        "    lstm_pred = lstm_model.predict(last_100)[0][0]\n",
        "\n",
        "    #XGBoost Prediction\n",
        "    xgb_input = data_scaled[-xgb_model.n_features_in_:].flatten().reshape(1, -1)\n",
        "    xgb_pred = xgb_model.predict(xgb_input)[0]\n",
        "\n",
        "    #ARIMA Prediction\n",
        "    arima_forecast = arima_model.forecast()\n",
        "    arima_pred = arima_forecast[0] if not isinstance(arima_forecast, pd.Series) else arima_forecast.iloc[0]\n",
        "\n",
        "    #Final Prediction Calculation\n",
        "    predicted_price = np.mean([lstm_pred, xgb_pred, arima_pred])\n",
        "    predicted_price = scaler.inverse_transform([[predicted_price]])[0][0]\n",
        "\n",
        "    return round(predicted_price, 2), data\n",
        "\n",
        "#Streamlit Web App (UI)\n",
        "st.set_page_config(page_title=\"AI Stock Predictor\", layout=\"wide\")\n",
        "\n",
        "st.title(\" Advanced AI-Based Stock Price Prediction\")\n",
        "st.sidebar.header(\"Enter Stock Symbol\")\n",
        "\n",
        "#User Input for Stock Symbol\n",
        "company = st.sidebar.text_input(\"Stock Symbol (e.g., AAPL, TSLA, MSFT)\", \"AAPL\").upper()\n",
        "\n",
        "if st.sidebar.button(\"Predict Stock Price\"):\n",
        "    st.subheader(f\"Stock Price Prediction for {company}\")\n",
        "    try:\n",
        "        predicted_price, stock_data = predict_stock(company)\n",
        "\n",
        "        # Display Predicted Price\n",
        "        st.metric(label=\"Predicted Price\", value=f\"${predicted_price}\")\n",
        "\n",
        "        # Plot Historical Stock Data (Candlestick Chart)\n",
        "        st.subheader(\"Historical Stock Prices (Real-Time)\")\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(go.Candlestick(\n",
        "            x=stock_data.index,\n",
        "            open=stock_data[\"Open\"],\n",
        "            high=stock_data[\"High\"],\n",
        "            low=stock_data[\"Low\"],\n",
        "            close=stock_data[\"Close\"],\n",
        "            name=\"Market Data\"\n",
        "        ))\n",
        "        fig.update_layout(title=f\"{company} Stock Price\", xaxis_title=\"Date\", yaxis_title=\"Price\", template=\"plotly_dark\")\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "    except ValueError as e:\n",
        "        st.error(str(e))\n",
        "\n",
        "st.sidebar.write(\"Developed using **Streamlit & AI Models**\")"
      ],
      "metadata": {
        "id": "JvfJ9ew8zPh9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMwBj9qwmfqXOugG43XGCp"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}